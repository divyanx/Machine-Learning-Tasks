{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing multinomial Naive Bayes classifier on ‘20 Newsgroups Dataset’"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import operator\n",
    "import os\n",
    "import nltk\n",
    "from string import punctuation\n",
    "import re\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get cwd \n",
    "cwd = os.getcwd()\n",
    "mypath = cwd + '/20_newsgroups/'\n",
    "folders = [f for f in os.listdir(mypath)]\n",
    "folders.sort()\n",
    "if folders[0] == \".DS_Store\":\n",
    "    folders.pop(0)\n",
    "folders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Words\n",
    "\n",
    "Stop words are words that show up a lot in every document (e.g. prepositions and pronouns). Since stop words are of no use for us we will not consider them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/divyansh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/divyansh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "punctuations=list(punctuation)\n",
    "stopWords=stopwords.words('english')\n",
    "stopWords+=punctuations \n",
    "stopWords= set(stopWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Vocabulary/Feature Set\n",
    "\n",
    "Vocabulary will contain words that will act as features for our model , we will take some amount of words from all documents sorted by frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90926"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocab_dict will be a dictionary of the form {word: frequency} over all documents\n",
    "\n",
    "data = {}\n",
    "vocab_dict = {}\n",
    "for folder in folders:\n",
    "    data[folder] = []\n",
    "    for doc in os.listdir(os.path.join(mypath, folder)):\n",
    "        with open(os.path.join(mypath, folder,doc), 'r', encoding='latin-1') as f:\n",
    "            text = f.read()\n",
    "            text = text.lower()\n",
    "            # remove any word that has a non alphabetical character\n",
    "            text = re.sub(r'[^a-z]', ' ', text)\n",
    "            temp=text.split()\n",
    "            data[folder].append(temp)\n",
    "            for token in temp:\n",
    "                if token in vocab_dict:\n",
    "                    vocab_dict[token] += 1\n",
    "                elif len(token) >=5 and token not in stopWords:\n",
    "                    vocab_dict[token] = 1\n",
    "                    \n",
    "            \n",
    "\n",
    "len(vocab_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Feature List\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'access',\n",
       " 'actually',\n",
       " 'agate',\n",
       " 'always',\n",
       " 'american',\n",
       " 'andrew',\n",
       " 'another',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'apple',\n",
       " 'around',\n",
       " 'article',\n",
       " 'atheism',\n",
       " 'athos',\n",
       " 'autos',\n",
       " 'available',\n",
       " 'baseball',\n",
       " 'based',\n",
       " 'believe',\n",
       " 'berkeley',\n",
       " 'better',\n",
       " 'called',\n",
       " 'cantaloupe',\n",
       " 'center',\n",
       " 'change',\n",
       " 'children',\n",
       " 'christian',\n",
       " 'clipper',\n",
       " 'colorado',\n",
       " 'columbia',\n",
       " 'computer',\n",
       " 'control',\n",
       " 'could',\n",
       " 'course',\n",
       " 'crabapple',\n",
       " 'crypt',\n",
       " 'culture',\n",
       " 'darwin',\n",
       " 'david',\n",
       " 'different',\n",
       " 'distribution',\n",
       " 'drive',\n",
       " 'either',\n",
       " 'electronics',\n",
       " 'email',\n",
       " 'enough',\n",
       " 'europa',\n",
       " 'every',\n",
       " 'evidence',\n",
       " 'example',\n",
       " 'files',\n",
       " 'first',\n",
       " 'following',\n",
       " 'followup',\n",
       " 'forsale',\n",
       " 'found',\n",
       " 'games',\n",
       " 'gatech',\n",
       " 'general',\n",
       " 'geneva',\n",
       " 'getting',\n",
       " 'given',\n",
       " 'going',\n",
       " 'government',\n",
       " 'graphics',\n",
       " 'great',\n",
       " 'group',\n",
       " 'gtefsd',\n",
       " 'hardware',\n",
       " 'harvard',\n",
       " 'heard',\n",
       " 'history',\n",
       " 'hockey',\n",
       " 'however',\n",
       " 'howland',\n",
       " 'human',\n",
       " 'image',\n",
       " 'information',\n",
       " 'internet',\n",
       " 'israel',\n",
       " 'jesus',\n",
       " 'jewish',\n",
       " 'keywords',\n",
       " 'least',\n",
       " 'lines',\n",
       " 'little',\n",
       " 'local',\n",
       " 'looking',\n",
       " 'magnesium',\n",
       " 'makes',\n",
       " 'maybe',\n",
       " 'means',\n",
       " 'message',\n",
       " 'michael',\n",
       " 'mideast',\n",
       " 'might',\n",
       " 'national',\n",
       " 'netcom',\n",
       " 'network',\n",
       " 'never',\n",
       " 'newsgroups',\n",
       " 'nothing',\n",
       " 'number',\n",
       " 'opinions',\n",
       " 'order',\n",
       " 'organization',\n",
       " 'others',\n",
       " 'people',\n",
       " 'person',\n",
       " 'phone',\n",
       " 'place',\n",
       " 'please',\n",
       " 'point',\n",
       " 'politics',\n",
       " 'possible',\n",
       " 'posting',\n",
       " 'power',\n",
       " 'president',\n",
       " 'probably',\n",
       " 'problem',\n",
       " 'problems',\n",
       " 'program',\n",
       " 'public',\n",
       " 'purdue',\n",
       " 'question',\n",
       " 'questions',\n",
       " 'quite',\n",
       " 'rather',\n",
       " 'really',\n",
       " 'reason',\n",
       " 'references',\n",
       " 'religion',\n",
       " 'remember',\n",
       " 'reply',\n",
       " 'research',\n",
       " 'reston',\n",
       " 'right',\n",
       " 'rights',\n",
       " 'rochester',\n",
       " 'rutgers',\n",
       " 'saying',\n",
       " 'science',\n",
       " 'second',\n",
       " 'security',\n",
       " 'seems',\n",
       " 'sender',\n",
       " 'server',\n",
       " 'service',\n",
       " 'several',\n",
       " 'since',\n",
       " 'software',\n",
       " 'someone',\n",
       " 'something',\n",
       " 'source',\n",
       " 'space',\n",
       " 'sport',\n",
       " 'standard',\n",
       " 'stanford',\n",
       " 'start',\n",
       " 'state',\n",
       " 'steve',\n",
       " 'still',\n",
       " 'subject',\n",
       " 'support',\n",
       " 'system',\n",
       " 'systems',\n",
       " 'technology',\n",
       " 'thanks',\n",
       " 'thing',\n",
       " 'things',\n",
       " 'think',\n",
       " 'though',\n",
       " 'thought',\n",
       " 'three',\n",
       " 'times',\n",
       " 'today',\n",
       " 'toronto',\n",
       " 'trying',\n",
       " 'turkish',\n",
       " 'university',\n",
       " 'usenet',\n",
       " 'using',\n",
       " 'utexas',\n",
       " 'uunet',\n",
       " 'version',\n",
       " 'virginia',\n",
       " 'wanted',\n",
       " 'washington',\n",
       " 'whether',\n",
       " 'window',\n",
       " 'windows',\n",
       " 'without',\n",
       " 'world',\n",
       " 'would',\n",
       " 'writes',\n",
       " 'wrong',\n",
       " 'wrote',\n",
       " 'wupost',\n",
       " 'years',\n",
       " 'zaphod'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort the dictionary based on frequency of each 'possible' vocabulary word\n",
    "sorted_vocab=sorted(vocab_dict.items(),key=operator.itemgetter(1),reverse=True)\n",
    "# Choosing top 2000 vocab words as features\n",
    "feature_list=[]\n",
    "for key in sorted_vocab:\n",
    "    feature_list.append(key[0])\n",
    "feature_list=feature_list[0:200] # K = 200 (number of words in vocab)\n",
    "feature_list = set(feature_list)\n",
    "feature_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data():\n",
    "    '''\n",
    "    return : a dataframe of columns as features and rows as documents\n",
    "    '''\n",
    "    df = pd.DataFrame(columns = feature_list)\n",
    "    Y=[]\n",
    "    for folder in folders:\n",
    "        print(\"|\",end=\"\")\n",
    "        for doc in data[folder]:\n",
    "            # print(\":\",end=\"\")\n",
    "            Y.append(folder)\n",
    "            # Add a new row for every file\n",
    "            df.loc[len(df)] = np.zeros(len(feature_list))\n",
    "            for txt in doc:\n",
    "                for word in txt.split():\n",
    "                    if word in feature_list:\n",
    "                        df.loc[len(df)-1,word] += 1\n",
    "        \n",
    "    # add Y as a column to dataframe\n",
    "    df['Y']=Y\n",
    "    return df\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Test Data and Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "||||||||||||||||||||"
     ]
    }
   ],
   "source": [
    "df = prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>probably</th>\n",
       "      <th>technology</th>\n",
       "      <th>american</th>\n",
       "      <th>subject</th>\n",
       "      <th>power</th>\n",
       "      <th>support</th>\n",
       "      <th>government</th>\n",
       "      <th>state</th>\n",
       "      <th>might</th>\n",
       "      <th>files</th>\n",
       "      <th>...</th>\n",
       "      <th>keywords</th>\n",
       "      <th>looking</th>\n",
       "      <th>history</th>\n",
       "      <th>least</th>\n",
       "      <th>second</th>\n",
       "      <th>today</th>\n",
       "      <th>clipper</th>\n",
       "      <th>columbia</th>\n",
       "      <th>course</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>alt.atheism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>alt.atheism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>alt.atheism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>alt.atheism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>alt.atheism</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 201 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   probably  technology  american  subject  power  support  government  state  \\\n",
       "0       0.0         0.0       0.0      1.0    0.0      0.0         0.0    2.0   \n",
       "1       0.0         0.0       0.0      1.0    0.0      0.0         0.0    1.0   \n",
       "2       0.0         0.0       0.0      1.0    0.0      0.0         0.0    0.0   \n",
       "3       0.0         1.0       0.0      1.0    0.0      0.0         0.0    0.0   \n",
       "4       0.0         0.0       0.0      1.0    0.0      0.0         0.0    1.0   \n",
       "\n",
       "   might  files  ...  keywords  looking  history  least  second  today  \\\n",
       "0    0.0    0.0  ...       0.0      0.0      0.0    0.0     0.0    0.0   \n",
       "1    1.0    0.0  ...       0.0      0.0      0.0    0.0     0.0    0.0   \n",
       "2    2.0    0.0  ...       0.0      1.0      0.0    0.0     0.0    0.0   \n",
       "3    0.0    0.0  ...       0.0      0.0      0.0    0.0     0.0    0.0   \n",
       "4    0.0    0.0  ...       0.0      0.0      0.0    0.0     0.0    0.0   \n",
       "\n",
       "   clipper  columbia  course            Y  \n",
       "0      0.0       0.0     0.0  alt.atheism  \n",
       "1      0.0       0.0     0.0  alt.atheism  \n",
       "2      0.0       1.0     1.0  alt.atheism  \n",
       "3      0.0       0.0     0.0  alt.atheism  \n",
       "4      0.0       0.0     0.0  alt.atheism  \n",
       "\n",
       "[5 rows x 201 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.drop(['Y'],axis=1).values\n",
    "#remove last column of X\n",
    "\n",
    "Y = df['Y'].values\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the last column of df\n",
    "last = df.iloc[:,-1]\n",
    "# divide each row of df by sum of all values in that row except the last column\n",
    "new_df = df.drop(['Y'],axis=1)\n",
    "new_df = new_df.div(new_df.sum(axis=1), axis=0)\n",
    "df = pd.concat([new_df, last], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test=train_test_split(X,Y,random_state=0,test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'talk.politics.misc': 741,\n",
       "         'talk.religion.misc': 764,\n",
       "         'sci.electronics': 756,\n",
       "         'misc.forsale': 739,\n",
       "         'rec.motorcycles': 716,\n",
       "         'comp.sys.mac.hardware': 764,\n",
       "         'rec.sport.hockey': 769,\n",
       "         'talk.politics.mideast': 719,\n",
       "         'soc.religion.christian': 745,\n",
       "         'sci.med': 744,\n",
       "         'comp.os.ms-windows.misc': 751,\n",
       "         'talk.politics.guns': 751,\n",
       "         'alt.atheism': 767,\n",
       "         'comp.graphics': 747,\n",
       "         'comp.sys.ibm.pc.hardware': 760,\n",
       "         'rec.sport.baseball': 752,\n",
       "         'sci.crypt': 767,\n",
       "         'rec.autos': 731,\n",
       "         'sci.space': 754,\n",
       "         'comp.windows.x': 760})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count each distinct element in the y_train\n",
    "from collections import Counter\n",
    "Counter(y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the inbuilt Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7642"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf=MultinomialNB()\n",
    "clf.fit(x_train,y_train)\n",
    "clf.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alt.atheism 0.05114356204574248\n",
      "comp.graphics 0.04980996199239848\n",
      "comp.os.ms-windows.misc 0.05007668200306728\n",
      "comp.sys.ibm.pc.hardware 0.05067680202707208\n",
      "comp.sys.mac.hardware 0.050943522037740885\n",
      "comp.windows.x 0.05067680202707208\n",
      "misc.forsale 0.04927652197106088\n",
      "rec.autos 0.04874308194972328\n",
      "rec.motorcycles 0.047742881909715276\n",
      "rec.sport.baseball 0.05014336200573448\n",
      "rec.sport.hockey 0.05127692205107688\n",
      "sci.crypt 0.05114356204574248\n",
      "sci.electronics 0.05041008201640328\n",
      "sci.med 0.04960992198439688\n",
      "sci.space 0.05027672201106888\n",
      "soc.religion.christian 0.04967660198706408\n",
      "talk.politics.guns 0.05007668200306728\n",
      "talk.politics.mideast 0.047942921917716874\n",
      "talk.politics.misc 0.04940988197639528\n",
      "talk.religion.misc 0.050943522037740885\n"
     ]
    }
   ],
   "source": [
    "# calculate priors for each type of folder\n",
    "priors ={}\n",
    "for folder in folders:\n",
    "    priors[folder] = len(y_train[y_train==folder]) / len(y_train)\n",
    "    print(folder,priors[folder])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CCD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.004518957759377265"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word= 'example'\n",
    "folder=folders[0]\n",
    "(df[word][df['Y']==folder].sum() + 1) / (len(df[df['Y']==folder]) + len(feature_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>probably</th>\n",
       "      <th>technology</th>\n",
       "      <th>american</th>\n",
       "      <th>subject</th>\n",
       "      <th>power</th>\n",
       "      <th>support</th>\n",
       "      <th>government</th>\n",
       "      <th>state</th>\n",
       "      <th>might</th>\n",
       "      <th>files</th>\n",
       "      <th>...</th>\n",
       "      <th>system</th>\n",
       "      <th>keywords</th>\n",
       "      <th>looking</th>\n",
       "      <th>history</th>\n",
       "      <th>least</th>\n",
       "      <th>second</th>\n",
       "      <th>today</th>\n",
       "      <th>clipper</th>\n",
       "      <th>columbia</th>\n",
       "      <th>course</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.002815</td>\n",
       "      <td>0.002969</td>\n",
       "      <td>0.001693</td>\n",
       "      <td>0.026948</td>\n",
       "      <td>0.002120</td>\n",
       "      <td>0.002765</td>\n",
       "      <td>0.001663</td>\n",
       "      <td>0.021100</td>\n",
       "      <td>0.003856</td>\n",
       "      <td>0.001132</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006697</td>\n",
       "      <td>0.001485</td>\n",
       "      <td>0.001620</td>\n",
       "      <td>0.002694</td>\n",
       "      <td>0.003231</td>\n",
       "      <td>0.002037</td>\n",
       "      <td>0.001790</td>\n",
       "      <td>0.000903</td>\n",
       "      <td>0.002181</td>\n",
       "      <td>0.003926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.002048</td>\n",
       "      <td>0.002884</td>\n",
       "      <td>0.001319</td>\n",
       "      <td>0.033009</td>\n",
       "      <td>0.001182</td>\n",
       "      <td>0.003349</td>\n",
       "      <td>0.000925</td>\n",
       "      <td>0.022417</td>\n",
       "      <td>0.002469</td>\n",
       "      <td>0.008202</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005849</td>\n",
       "      <td>0.005926</td>\n",
       "      <td>0.007169</td>\n",
       "      <td>0.000883</td>\n",
       "      <td>0.002015</td>\n",
       "      <td>0.001788</td>\n",
       "      <td>0.001179</td>\n",
       "      <td>0.000943</td>\n",
       "      <td>0.003427</td>\n",
       "      <td>0.002181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.002324</td>\n",
       "      <td>0.003116</td>\n",
       "      <td>0.001571</td>\n",
       "      <td>0.030831</td>\n",
       "      <td>0.001165</td>\n",
       "      <td>0.003080</td>\n",
       "      <td>0.000987</td>\n",
       "      <td>0.021891</td>\n",
       "      <td>0.002201</td>\n",
       "      <td>0.009495</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009870</td>\n",
       "      <td>0.003932</td>\n",
       "      <td>0.003673</td>\n",
       "      <td>0.000870</td>\n",
       "      <td>0.002044</td>\n",
       "      <td>0.001581</td>\n",
       "      <td>0.001074</td>\n",
       "      <td>0.000942</td>\n",
       "      <td>0.003674</td>\n",
       "      <td>0.001391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.002585</td>\n",
       "      <td>0.003457</td>\n",
       "      <td>0.001132</td>\n",
       "      <td>0.032156</td>\n",
       "      <td>0.003883</td>\n",
       "      <td>0.003982</td>\n",
       "      <td>0.000944</td>\n",
       "      <td>0.024436</td>\n",
       "      <td>0.003074</td>\n",
       "      <td>0.002778</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013153</td>\n",
       "      <td>0.004035</td>\n",
       "      <td>0.003246</td>\n",
       "      <td>0.000859</td>\n",
       "      <td>0.002502</td>\n",
       "      <td>0.002889</td>\n",
       "      <td>0.001132</td>\n",
       "      <td>0.000863</td>\n",
       "      <td>0.004353</td>\n",
       "      <td>0.001601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.002536</td>\n",
       "      <td>0.003397</td>\n",
       "      <td>0.001493</td>\n",
       "      <td>0.031956</td>\n",
       "      <td>0.006675</td>\n",
       "      <td>0.003033</td>\n",
       "      <td>0.000987</td>\n",
       "      <td>0.027947</td>\n",
       "      <td>0.003085</td>\n",
       "      <td>0.001612</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012864</td>\n",
       "      <td>0.002850</td>\n",
       "      <td>0.003039</td>\n",
       "      <td>0.000833</td>\n",
       "      <td>0.002196</td>\n",
       "      <td>0.002171</td>\n",
       "      <td>0.001306</td>\n",
       "      <td>0.000833</td>\n",
       "      <td>0.002455</td>\n",
       "      <td>0.001900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   probably  technology  american   subject     power   support  government  \\\n",
       "0  0.002815    0.002969  0.001693  0.026948  0.002120  0.002765    0.001663   \n",
       "1  0.002048    0.002884  0.001319  0.033009  0.001182  0.003349    0.000925   \n",
       "2  0.002324    0.003116  0.001571  0.030831  0.001165  0.003080    0.000987   \n",
       "3  0.002585    0.003457  0.001132  0.032156  0.003883  0.003982    0.000944   \n",
       "4  0.002536    0.003397  0.001493  0.031956  0.006675  0.003033    0.000987   \n",
       "\n",
       "      state     might     files  ...    system  keywords   looking   history  \\\n",
       "0  0.021100  0.003856  0.001132  ...  0.006697  0.001485  0.001620  0.002694   \n",
       "1  0.022417  0.002469  0.008202  ...  0.005849  0.005926  0.007169  0.000883   \n",
       "2  0.021891  0.002201  0.009495  ...  0.009870  0.003932  0.003673  0.000870   \n",
       "3  0.024436  0.003074  0.002778  ...  0.013153  0.004035  0.003246  0.000859   \n",
       "4  0.027947  0.003085  0.001612  ...  0.012864  0.002850  0.003039  0.000833   \n",
       "\n",
       "      least    second     today   clipper  columbia    course  \n",
       "0  0.003231  0.002037  0.001790  0.000903  0.002181  0.003926  \n",
       "1  0.002015  0.001788  0.001179  0.000943  0.003427  0.002181  \n",
       "2  0.002044  0.001581  0.001074  0.000942  0.003674  0.001391  \n",
       "3  0.002502  0.002889  0.001132  0.000863  0.004353  0.001601  \n",
       "4  0.002196  0.002171  0.001306  0.000833  0.002455  0.001900  \n",
       "\n",
       "[5 rows x 200 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate class conditional probabilities p(x|y) where x is a word and y is a newsgroup for each word in the vocabulary as a dataframe\n",
    "conditional_probabilities = pd.DataFrame(columns=feature_list)\n",
    "i=0;\n",
    "for folder in folders:\n",
    "    # add a new row for each folder\n",
    "    conditional_probabilities.loc[len(conditional_probabilities)] = np.zeros(len(feature_list))\n",
    "    # use df calculated above to calculate conditional probabilities\n",
    "    for word in feature_list:\n",
    "        #print(word)\n",
    "        conditional_probabilities[word][i] = (df[word][df['Y']==folder].sum() + 1) / (len(df[df['Y']==folder]) + len(feature_list))\n",
    "    i+=1\n",
    "conditional_probabilities.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "probably      1.0\n",
       "technology    1.0\n",
       "american      1.0\n",
       "subject       1.0\n",
       "power         1.0\n",
       "             ... \n",
       "second        1.0\n",
       "today         1.0\n",
       "clipper       1.0\n",
       "columbia      1.0\n",
       "course        1.0\n",
       "Length: 200, dtype: float64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalize each columns of conditional_probabilities\n",
    "conditional_probabilities = conditional_probabilities.div(conditional_probabilities.sum(axis=0), axis=1)\n",
    "# get the sum of each column\n",
    "conditional_probabilities.sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a={'a':1,'b':2,'c':3}\n",
    "max(a,key=a.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    # calculate the probability of each word in the text belonging to each newsgroup\n",
    "    probabilities = {}\n",
    "    i=0\n",
    "    for f_in in range(len(folders)):\n",
    "        probabilities[folders[f_in]] = np.log(priors[folders[f_in]])\n",
    "        for i in range(len(text)):\n",
    "            probabilities[folders[f_in]] += np.log(conditional_probabilities.values[f_in][i])*text[i]\n",
    "    # return the newsgroup with the highest probability\n",
    "    # print(probabilities)\n",
    "    return max(probabilities, key=probabilities.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_data(x_test):\n",
    "    predictions = []\n",
    "    for i in range(len(x_test)):\n",
    "        predictions.append(predict(x_test[i]))\n",
    "    return predictions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "y_pred=predict_data(x_test)\n",
    "# print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7904"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_pred==y_test)/len(y_test)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "74fc0cb1932bf4b202d61f4ecf9d651fa79ff967900f756d776817a87c2d0209"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
